{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='1570cea6-ac60-458f-9709-a50076c71bde'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 348, 'prompt_tokens': 271, 'total_tokens': 619, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjBCSWM1oyiHY0tcdx6T7ftDqgPpB', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--b37e5f19-eefe-4ef1-bd32-b5fcae1a5ad7-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_CqnDH76NTtSyM2q1QCYlDGX2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 271, 'output_tokens': 348, 'total_tokens': 619, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 320}}),\n",
      "              ToolMessage(content='{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"Sign up for our newsletter to stay up to date **DATE:** # MCP Adapters for LangChain and LangGraph **DATE:** **AUTHOR:** The LangChain Team The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools** * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage.\",\\n      \"score\": 0.94620544,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `langchain-mcp-adapters` library. from langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClient from langchain.agents import create_agent from langchain.agents import  create_agent client = MultiServerMCPClient( client = MultiServerMCPClient(  { { \\\\\"math\\\\\": { \\\\\"math\\\\\": { \\\\\"transport\\\\\": \\\\\"stdio\\\\\", # Local subprocess communication  \\\\\"transport\\\\\": \\\\\"stdio\\\\\", # Local subprocess communication \\\\\"command\\\\\": \\\\\"python\\\\\",  \\\\\"command\\\\\": \\\\\"python\\\\\", # Absolute path to your math_server.py file # Absolute path to your math_server.py file \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"],  \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"], }, }, \\\\\"weather\\\\\": { \\\\\"weather\\\\\": { \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", # HTTP-based remote server  \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", # HTTP-based remote server  # Ensure you start your weather server on port 8000  # Ensure you start your weather server on port 8000 \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\",  \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\", } } } })) tools = await client.get_tools() tools =  await client.get_tools() agent = create_agent(agent = create_agent( \\\\\"claude-sonnet-4-5-20250929\\\\\", \\\\\"claude-sonnet-4-5-20250929\\\\\",  tools  tools ))math_response = await agent.ainvoke(math_response =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]} {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]}))weather_response = await agent.ainvoke(weather_response =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what is the weather in nyc?\\\\\"}]} {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what is the weather in nyc?\\\\\"}]}))\",\\n      \"score\": 0.8497846,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"streamable_http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\"\",\\n      \"score\": 0.7913865,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"Client for connecting to multiple MCP servers and loading LangChain tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. load_mcp_tools( load_mcp_tools( session: ClientSession | None, session: ClientSession ClientSession | None, *, *, connection: Connection | None = None, connection: Connection Connection | None = None, callbacks: Callbacks | None = None, callbacks: Callbacks Callbacks | None = None, tool_interceptors: list[ToolCallInterceptor] | None = None, tool_interceptors: list[ToolCallInterceptor ToolCallInterceptor] | None = None, server_name: str | None = None, server_name: str | None = None,) -> list[) -> list[langchain.tools.BaseTool]\",\\n      \"score\": 0.78846955,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://latenode.com/blog/ai-frameworks-technical-infrastructure/langchain-setup-tools-agents-memory/langchain-mcp-integration-complete-guide-to-mcp-adapters\",\\n      \"title\": \"LangChain MCP Integration: Complete Guide to MCP Adapters\",\\n      \"content\": \"from langchain_mcp import MCPAdapter from langchain_core.agents import create_react_agent from langchain_openai import ChatOpenAI # Database MCP server integration db_adapter = MCPAdapter( server_command=[\\\\\"python\\\\\", \\\\\"database_mcp_server.py\\\\\"], transport_type=\\\\\"stdio\\\\\", environment={ \\\\\"DATABASE_URL\\\\\": \\\\\"postgresql://user:pass@localhost:5432/mydb\\\\\", \\\\\"MAX_CONNECTIONS\\\\\": \\\\\"10\\\\\" } ) await db_adapter.connect() db_tools = await db_adapter.get_tools() # Create an agent with database capabilities llm = ChatOpenAI(model=\\\\\"gpt-4\\\\\") agent = create_react_agent(llm, db_tools) # Execute SQL queries through MCP response = await agent.ainvoke({ \\\\\"input\\\\\": \\\\\"Find all customers who made purchases over $500 in the last month\\\\\" })  Instead of writing adapter code or managing MCP servers, Latenode users can connect AI agents to more than 350 external services using pre-built connectors and drag-and-drop workflows. ### How does Latenode make it easier to connect AI agents to external tools compared to LangChain MCP adapters?\",\\n      \"score\": 0.784336,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.74,\\n  \"request_id\": \"83b404f3-9493-4962-a35b-debfb74bd3b4\"\\n}', name='search_web', id='c8d7c00d-f192-43c0-9ba1-d0c35fcd292e', tool_call_id='call_CqnDH76NTtSyM2q1QCYlDGX2'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 916, 'prompt_tokens': 2062, 'total_tokens': 2978, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 832, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjBCZvDfhfv6FNim2IMiFoYTQaxvK', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--87584a82-e0fd-4780-b005-d94dbcec56c9-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters install'}, 'id': 'call_urENUHvRRDO8ejYsFyuxG2Ms', 'type': 'tool_call'}, {'name': 'search_web', 'args': {'query': 'MultiServerMCPClient LangChain'}, 'id': 'call_V9oGEDk4GcTVBdkEFVeR2cyc', 'type': 'tool_call'}, {'name': 'search_web', 'args': {'query': 'MCP LangChain adapters'}, 'id': 'call_ppYx82NaDvdMv4oqpiv2iyIp', 'type': 'tool_call'}], usage_metadata={'input_tokens': 2062, 'output_tokens': 916, 'total_tokens': 2978, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 832}}),\n",
      "              ToolMessage(content='{\\n  \"query\": \"langchain-mcp-adapters install\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://anaconda.org/conda-forge/langchain-mcp-adapters\",\\n      \"title\": \"langchain-mcp-adapters - Conda - Anaconda.org\",\\n      \"content\": \"Installation. To install this package, run one of the following: Conda. $ conda install conda-forge::langchain-mcp-adapters. Usage Tracking.\",\\n      \"score\": 0.8479807,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"Sign up for our newsletter to stay up to date **DATE:** # MCP Adapters for LangChain and LangGraph **DATE:** **AUTHOR:** The LangChain Team The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools** * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage.\",\\n      \"score\": 0.8098936,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.youtube.com/watch?v=-8DVauf5iu4\",\\n      \"title\": \"Pip Install LangChain MCP Adapters | Python - YouTube\",\\n      \"content\": \"In this video, we\\'ll explore how to install and use LangChain MCP Adapters ‚Äî a powerful way to connect your LangChain agents with external\",\\n      \"score\": 0.75049835,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://composio.dev/blog/langchain-mcp-adapter-a-step-by-step-guide-to-build-mcp-agents\",\\n      \"title\": \"LangChain MCP Adapter: A step-by-step guide to build MCP Agents\",\\n      \"content\": \"import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{createReactAgent} from\\\\\"@langchain/langgraph/prebuilt\\\\\"; import{HumanMessage, AIMessage} from\\\\\"@langchain/core/messages\\\\\"; import dotenv from \\\\\"dotenv\\\\\"; import* as readline from\\\\\"node:readline/promises\\\\\"; import{stdin as input, stdout as output} from\\\\\"node:process\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\"; dotenv config();// üß© Add any MCP tool URLs you‚Äôve set up with Composio const mcpServers{// Example:// gmail: {// transport: \\\\\"sse\\\\\",// url: \\\\\"\\\\\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\\\\\"gpt-4o\\\\\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\\\\\"Agent is ready. import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{createReactAgent} from\\\\\"@langchain/langgraph/prebuilt\\\\\"; import{HumanMessage, AIMessage} from\\\\\"@langchain/core/messages\\\\\"; import dotenv from \\\\\"dotenv\\\\\"; import* as readline from\\\\\"node:readline/promises\\\\\"; import{stdin as input, stdout as output} from\\\\\"node:process\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\"; dotenv config();// üß© Add any MCP tool URLs you‚Äôve set up with Composio const mcpServers{// Example:// gmail: {// transport: \\\\\"sse\\\\\",// url: \\\\\"\\\\\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\\\\\"gpt-4o\\\\\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\\\\\"Agent is ready.\",\\n      \"score\": 0.6208291,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\";import { createAgent } from \\\\\"langchain\\\\\"; import { createAgent } from  \\\\\"langchain\\\\\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \\\\\"stdio\\\\\", // Local subprocess communication transport:  \\\\\"stdio\\\\\", // Local subprocess communication command: \\\\\"node\\\\\", command:  \\\\\"node\\\\\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\\\\\"/path/to/math_server.js\\\\\"], args: [\\\\\"/path/to/math_server.js\\\\\"], }, }, weather: { weather: { transport: \\\\\"sse\\\\\", // Server-Sent Events for streaming transport:  \\\\\"sse\\\\\", // Server-Sent Events for streaming // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \\\\\"http://localhost:8000/mcp\\\\\", url: \\\\\"http://localhost:8000/mcp\\\\\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.\",\\n      \"score\": 0.5829428,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.82,\\n  \"request_id\": \"40d8c935-cc4c-4aa7-b67d-233b80eafce2\"\\n}', name='search_web', id='4ead390a-627b-480c-927b-0eafdb3106e6', tool_call_id='call_urENUHvRRDO8ejYsFyuxG2Ms'),\n",
      "              ToolMessage(content='{\\n  \"query\": \"MultiServerMCPClient LangChain\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"Client for connecting to multiple MCP servers and loading LangChain tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. load_mcp_tools( load_mcp_tools( session: ClientSession | None, session: ClientSession ClientSession | None, *, *, connection: Connection | None = None, connection: Connection Connection | None = None, callbacks: Callbacks | None = None, callbacks: Callbacks Callbacks | None = None, tool_interceptors: list[ToolCallInterceptor] | None = None, tool_interceptors: list[ToolCallInterceptor ToolCallInterceptor] | None = None, server_name: str | None = None, server_name: str | None = None,) -> list[) -> list[langchain.tools.BaseTool]\",\\n      \"score\": 0.8679453,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `langchain-mcp-adapters` library. from langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClient from langchain.agents import create_agent from langchain.agents import  create_agent client = MultiServerMCPClient( client = MultiServerMCPClient(  { { \\\\\"math\\\\\": { \\\\\"math\\\\\": { \\\\\"transport\\\\\": \\\\\"stdio\\\\\", # Local subprocess communication  \\\\\"transport\\\\\": \\\\\"stdio\\\\\", # Local subprocess communication \\\\\"command\\\\\": \\\\\"python\\\\\",  \\\\\"command\\\\\": \\\\\"python\\\\\", # Absolute path to your math_server.py file # Absolute path to your math_server.py file \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"],  \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"], }, }, \\\\\"weather\\\\\": { \\\\\"weather\\\\\": { \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", # HTTP-based remote server  \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", # HTTP-based remote server  # Ensure you start your weather server on port 8000  # Ensure you start your weather server on port 8000 \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\",  \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\", } } } })) tools = await client.get_tools() tools =  await client.get_tools() agent = create_agent(agent = create_agent( \\\\\"claude-sonnet-4-5-20250929\\\\\", \\\\\"claude-sonnet-4-5-20250929\\\\\",  tools  tools ))math_response = await agent.ainvoke(math_response =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]} {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]}))weather_response = await agent.ainvoke(weather_response =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what is the weather in nyc?\\\\\"}]} {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what is the weather in nyc?\\\\\"}]}))\",\\n      \"score\": 0.86767644,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\";import { createAgent } from \\\\\"langchain\\\\\"; import { createAgent } from  \\\\\"langchain\\\\\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \\\\\"stdio\\\\\", // Local subprocess communication transport:  \\\\\"stdio\\\\\", // Local subprocess communication command: \\\\\"node\\\\\", command:  \\\\\"node\\\\\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\\\\\"/path/to/math_server.js\\\\\"], args: [\\\\\"/path/to/math_server.js\\\\\"], }, }, weather: { weather: { transport: \\\\\"sse\\\\\", // Server-Sent Events for streaming transport:  \\\\\"sse\\\\\", // Server-Sent Events for streaming // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \\\\\"http://localhost:8000/mcp\\\\\", url: \\\\\"http://localhost:8000/mcp\\\\\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.\",\\n      \"score\": 0.8332299,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters\",\\n      \"content\": \"import{createAgent} from \\\\\"langchain\\\\\"; import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\";// Create client and connect to server const client = new MultiServerMCPClient({// Global tool configuration options// Whether to throw on errors if a tool fails to load (optional, default: true) throwOnLoadError true,// Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName false,// Optional additional prefix for tool names (optional, default: \\\\\"\\\\\") additionalToolNamePrefix \\\\\"\\\\\",// Use standardized content block format in tool outputs useStandardContentBlocks true,// Server configuration mcpServers{// adds a STDIO connection to a server named \\\\\"math\\\\\" math{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-math\\\\\"],// Restart configuration for stdio transport restart{enabled true, maxAttempts 3, delayMs 1000,},},// here\\'s a filesystem server filesystem{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-filesystem\\\\\"],},// Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled) weather{url\\\\\"https://example.com/weather/mcp\\\\\", headers{Authorization \\\\\"Bearer token123\\\\\",} automaticSSEFallback false},// OAuth 2.0 authentication (recommended for secure servers)\\\\\"oauth-protected-server\\\\\"{url\\\\\"https://protected.example.com/mcp\\\\\", authProvider new MyOAuthProvider({// Your OAuth provider implementation redirectUrl\\\\\"https://myapp.com/oauth/callback\\\\\", clientMetadata{redirect_uris[\\\\\"https://myapp.com/oauth/callback\\\\\"], client_name \\\\\"My MCP Client\\\\\", scope\\\\\"mcp:read mcp:write\\\\\"}}),// Can still include custom headers for non-auth purposes headers{\\\\\"User-Agent\\\\\"\\\\\"My-MCP-Client/1.0\\\\\"}},// how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure) github{transport \\\\\"sse\\\\\",// also works with \\\\\"type\\\\\" field instead of \\\\\"transport\\\\\" url\\\\\"https://example.com/mcp\\\\\", reconnect{enabled true, maxAttempts 5, delayMs 2000,},},},}); const tools = await client.\",\\n      \"score\": 0.8252664,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain üîå MCP\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"streamable_http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\"\",\\n      \"score\": 0.77893573,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.8,\\n  \"request_id\": \"7d32fbe5-6812-4f84-90fe-f34ba084812a\"\\n}', name='search_web', id='06e392e2-2260-4fc2-9b46-743a46505642', tool_call_id='call_V9oGEDk4GcTVBdkEFVeR2cyc'),\n",
      "              ToolMessage(content='{\\n  \"query\": \"MCP LangChain adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"Sign up for our newsletter to stay up to date **DATE:** # MCP Adapters for LangChain and LangGraph **DATE:** **AUTHOR:** The LangChain Team The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools** * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage.\",\\n      \"score\": 0.99997604,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain MCP - GitHub\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"streamable_http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\"\",\\n      \"score\": 0.99990535,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `langchain-mcp-adapters` library. from langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClient from langchain.agents import create_agent from langchain.agents import  create_agent client = MultiServerMCPClient( client = MultiServerMCPClient(  { { \\\\\"math\\\\\": { \\\\\"math\\\\\": { \\\\\"transport\\\\\": \\\\\"stdio\\\\\", # Local subprocess communication  \\\\\"transport\\\\\": \\\\\"stdio\\\\\", # Local subprocess communication \\\\\"command\\\\\": \\\\\"python\\\\\",  \\\\\"command\\\\\": \\\\\"python\\\\\", # Absolute path to your math_server.py file # Absolute path to your math_server.py file \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"],  \\\\\"args\\\\\": [\\\\\"/path/to/math_server.py\\\\\"], }, }, \\\\\"weather\\\\\": { \\\\\"weather\\\\\": { \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", # HTTP-based remote server  \\\\\"transport\\\\\": \\\\\"streamable_http\\\\\", # HTTP-based remote server  # Ensure you start your weather server on port 8000  # Ensure you start your weather server on port 8000 \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\",  \\\\\"url\\\\\": \\\\\"http://localhost:8000/mcp\\\\\", } } } })) tools = await client.get_tools() tools =  await client.get_tools() agent = create_agent(agent = create_agent( \\\\\"claude-sonnet-4-5-20250929\\\\\", \\\\\"claude-sonnet-4-5-20250929\\\\\",  tools  tools ))math_response = await agent.ainvoke(math_response =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]} {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what\\'s (3 + 5) x 12?\\\\\"}]}))weather_response = await agent.ainvoke(weather_response =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what is the weather in nyc?\\\\\"}]} {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"what is the weather in nyc?\\\\\"}]}))\",\\n      \"score\": 0.9998591,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"import{createAgent} from \\\\\"langchain\\\\\"; import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\";// Create client and connect to server const client = new MultiServerMCPClient({// Global tool configuration options// Whether to throw on errors if a tool fails to load (optional, default: true) throwOnLoadError true,// Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName false,// Optional additional prefix for tool names (optional, default: \\\\\"\\\\\") additionalToolNamePrefix \\\\\"\\\\\",// Use standardized content block format in tool outputs useStandardContentBlocks true,// Server configuration mcpServers{// adds a STDIO connection to a server named \\\\\"math\\\\\" math{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-math\\\\\"],// Restart configuration for stdio transport restart{enabled true, maxAttempts 3, delayMs 1000,},},// here\\'s a filesystem server filesystem{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-filesystem\\\\\"],},// Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled) weather{url\\\\\"https://example.com/weather/mcp\\\\\", headers{Authorization \\\\\"Bearer token123\\\\\",} automaticSSEFallback false},// OAuth 2.0 authentication (recommended for secure servers)\\\\\"oauth-protected-server\\\\\"{url\\\\\"https://protected.example.com/mcp\\\\\", authProvider new MyOAuthProvider({// Your OAuth provider implementation redirectUrl\\\\\"https://myapp.com/oauth/callback\\\\\", clientMetadata{redirect_uris[\\\\\"https://myapp.com/oauth/callback\\\\\"], client_name \\\\\"My MCP Client\\\\\", scope\\\\\"mcp:read mcp:write\\\\\"}}),// Can still include custom headers for non-auth purposes headers{\\\\\"User-Agent\\\\\"\\\\\"My-MCP-Client/1.0\\\\\"}},// how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure) github{transport \\\\\"sse\\\\\",// also works with \\\\\"type\\\\\" field instead of \\\\\"transport\\\\\" url\\\\\"https://example.com/mcp\\\\\", reconnect{enabled true, maxAttempts 5, delayMs 2000,},},},}); const tools = await client.\",\\n      \"score\": 0.99984527,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://composio.dev/blog/langchain-mcp-adapter-a-step-by-step-guide-to-build-mcp-agents\",\\n      \"title\": \"LangChain MCP Adapter: A step-by-step guide to build MCP Agents\",\\n      \"content\": \"import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{createReactAgent} from\\\\\"@langchain/langgraph/prebuilt\\\\\"; import{HumanMessage, AIMessage} from\\\\\"@langchain/core/messages\\\\\"; import dotenv from \\\\\"dotenv\\\\\"; import* as readline from\\\\\"node:readline/promises\\\\\"; import{stdin as input, stdout as output} from\\\\\"node:process\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\"; dotenv config();// üß© Add any MCP tool URLs you‚Äôve set up with Composio const mcpServers{// Example:// gmail: {// transport: \\\\\"sse\\\\\",// url: \\\\\"\\\\\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\\\\\"gpt-4o\\\\\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\\\\\"Agent is ready. import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{createReactAgent} from\\\\\"@langchain/langgraph/prebuilt\\\\\"; import{HumanMessage, AIMessage} from\\\\\"@langchain/core/messages\\\\\"; import dotenv from \\\\\"dotenv\\\\\"; import* as readline from\\\\\"node:readline/promises\\\\\"; import{stdin as input, stdout as output} from\\\\\"node:process\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\"; dotenv config();// üß© Add any MCP tool URLs you‚Äôve set up with Composio const mcpServers{// Example:// gmail: {// transport: \\\\\"sse\\\\\",// url: \\\\\"\\\\\"// }}; async function runChat(){const rl readline createInterface({input, output}); const chatHistory[]; const client new MultiServerMCPClient(mcpServers); const tools await client getTools(); const model new ChatOpenAI({modelName:\\\\\"gpt-4o\\\\\", temperature: 0, openAIApiKey: process env OPENAI_API_KEY}); const agent createReactAgent({llm: model, tools}); console log(\\\\\"Agent is ready.\",\\n      \"score\": 0.99981767,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.65,\\n  \"request_id\": \"086e2554-a802-4d8b-9338-1de8786ec708\"\\n}', name='search_web', id='8fb62c4a-fc0e-481f-8e65-80f73aa1a4c0', tool_call_id='call_ppYx82NaDvdMv4oqpiv2iyIp'),\n",
      "              AIMessage(content='Here‚Äôs a concise overview of the LangChain MCP Adapters library.\\n\\nWhat it is\\n- A bridge between Anthropic‚Äôs Model Context Protocol (MCP) tool servers and LangChain/LangGraph.\\n- It converts MCP tools into LangChain-compatible tools and lets LangChain agents pull from multiple MCP servers at once.\\n- Designed to help you tap into the wide MCP ecosystem (hundreds of tool servers) from LangChain and LangGraph workflows.\\n\\nKey concepts and capabilities\\n- MultiServerMCPClient: Central entrypoint to connect to multiple MCP servers and load their tools/prompts/resources.\\n- Server transports: Supports different transports (for example stdio for local subprocesses, streamable_http for HTTP-based servers, SSE, etc.).\\n- Tool loading: Load MCP-defined tools into LangChain as BaseTool-compatible entities.\\n- Server aggregation: Agent can combine tools from multiple MCP servers to create richer toolkits.\\n- LangChain integration: Works with LangChain agents and LangGraph workflows to enable MCP-powered tool usage inside conversations and tasks.\\n\\nWhere to learn more (docs, repo)\\n- Model Context Protocol (MCP) docs: Introduction to MCP and how to use MCP tools with LangChain via the adapters.\\n- langchain-mcp-adapters docs: API reference for MultiServerMCPClient, loading tools, sessions, and loading tools from MCP servers.\\n- GitHub repository: langchain-ai/langchain-mcp-adapters (source, examples, and usage patterns).\\n- Changelog/announcements: ‚ÄúMCP Adapters for LangChain and LangGraph‚Äù page with overview and motivation.\\n\\nInstallation options\\n- Python (LangChain ecosystem):\\n  - pip: pip install langchain-mcp-adapters\\n  - Python module path used in code: langchain_mcp_adapters\\n- JavaScript/Node.js:\\n  - npm: npm i @langchain/mcp-adapters\\n  - Module path used in code: @langchain/mcp-adapters\\n- Other packaging:\\n  - Conda (conda-forge): conda install -c conda-forge langchain-mcp-adapters\\n\\nMinimal usage outline (high level)\\n- Create an MCP client and point it at your MCP servers:\\n  - Python: MultiServerMCPClient with a dict of MCP servers and their transport configs.\\n  - JavaScript: new MultiServerMCPClient({ mcpServers: { ... } })\\n- Load tools from all connected MCP servers:\\n  - Tools = await client.get_tools()\\n- Create a LangChain agent using a language model and the loaded MCP tools:\\n  - Python: create_agent(llm=..., tools)\\n  - JavaScript: createAgent({ llm, tools })\\n- Invoke the agent with a user message to use MCP-powered tools as part of its reasoning:\\n  - Python: await agent.ainvoke({ messages: [{ role: \"user\", content: \"...\" }] })\\n  - JavaScript: await agent.invoke(...) (or equivalent in the JS API)\\n\\nWhat you can build with it\\n- Agents that call out to external MCP tool servers (math calculators, weather services, databases, file systems, etc.) as part of natural language conversations.\\n- Systems that orchestrate tools from multiple MCP servers to provide composite capabilities (e.g., math plus weather plus DB lookups) in a single agent session.\\n\\nIf you‚Äôd like, I can pull the exact, up-to-date codeSample from the docs and provide a ready-to-run snippet for either Python or JavaScript, tailored to your MCP server setup (e.g., local stdio server or HTTP-based MCP service).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 3043, 'prompt_tokens': 7839, 'total_tokens': 10882, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 2304, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1920}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjBCko5cngXMV0xa6t0ovqLuT1AIu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--5d6654f5-09f6-422d-aed0-72752679410a-0', usage_metadata={'input_tokens': 7839, 'output_tokens': 3043, 'total_tokens': 10882, 'input_token_details': {'audio': 0, 'cache_read': 1920}, 'output_token_details': {'audio': 0, 'reasoning': 2304}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='1ab2eeaf-a99f-4254-8bfa-9c8ca2093810'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 219, 'prompt_tokens': 296, 'total_tokens': 515, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjBF5TsMRVE8M17Erj8A1tOfTjSFr', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--b4386bdc-14a2-4a92-b999-29d0efabd359-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_Z6k05uJ3CfrqMa712elMfswu', 'type': 'tool_call'}], usage_metadata={'input_tokens': 296, 'output_tokens': 219, 'total_tokens': 515, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}),\n",
      "              ToolMessage(content='{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2025-12-04T16:33:33-05:00\",\\n  \"day_of_week\": \"Thursday\",\\n  \"is_dst\": false\\n}', name='get_current_time', id='2216d396-0f2d-41f0-b047-831189325f50', tool_call_id='call_Z6k05uJ3CfrqMa712elMfswu'),\n",
      "              AIMessage(content='Current time in New York (America/New_York): Thursday, December 4, 2025, 16:33:33 (4:33 PM) EST (UTC-5).\\n\\nWant me to convert to another time zone or do anything else with this time?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 448, 'prompt_tokens': 379, 'total_tokens': 827, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-CjBF8DjKi4aTQQrgKyOZc4JNu3W5J', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--87d0b019-6d87-4ac1-9fd4-cd07e188f432-0', usage_metadata={'input_tokens': 379, 'output_tokens': 448, 'total_tokens': 827, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
