{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='0c324b67-5ff4-4297-890c-bc68a29b35ad'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 271, 'total_tokens': 427, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7hcMCgTGy0DrXOYEXTxrtApYyw9q', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c47a2-1050-70a0-afee-ca80bbb14338-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_N9wuwBhgkIOAT2QxXtMjPKqk', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 271, 'output_tokens': 156, 'total_tokens': 427, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.99999535,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://composio.dev/blog/langchain-mcp-adapter-a-step-by-step-guide-to-build-mcp-agents\",\\n      \"title\": \"LangChain MCP Adapter: A step-by-step guide to build MCP Agents\",\\n      \"content\": \"The LangChain MCP adapter lets agents connect to any server that follows the Model Context Protocol, including Composio\\'s managed MCP server.\",\\n      \"score\": 0.9999864,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"This module provides functionality to convert MCP prompt messages into LangChain message objects, handling both user and assistant message types. FUNCTION\",\\n      \"score\": 0.9999852,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \\\\\"working-server\\\\\".\",\\n      \"score\": 0.99997294,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/python/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"from dataclasses import dataclass from  dataclasses import  dataclassfrom langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClientfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest from langchain_mcp_adapters.interceptors import  MCPToolCallRequestfrom langchain.agents import create_agent from langchain.agents import  create_agent @dataclass @dataclassclass Context: class  Context: user_id: str user_id: str api_key: str api_key: str async def inject_user_context(async  def  inject_user_context( request: MCPToolCallRequest,  request: MCPToolCallRequest, handler,  handler,):): \\\\\"\\\\\"\\\\\"Inject user credentials into MCP tool calls.\\\\\"\\\\\"\\\\\" \\\\\"\\\\\"\\\\\"Inject user credentials into MCP tool calls.\\\\\"\\\\\"\\\\\" runtime = request.runtime  runtime = request.runtime user_id = runtime.context.user_id  user_id = runtime.context.user_id  api_key = runtime.context.api_key  api_key = runtime.context.api_key   # Add user context to tool arguments  # Add user context to tool arguments modified_request = request.override( modified_request = request.override( args={**request.args, \\\\\"user_id\\\\\": user_id}  args ={**request.args, \\\\\"user_id\\\\\": user_id} ) ) return await handler(modified_request)  return  await handler(modified_request) client = MultiServerMCPClient(client = MultiServerMCPClient( {...}, {...}, tool_interceptors=[inject_user_context],  tool_interceptors =[inject_user_context],))tools = await client.get_tools() tools =  await client.get_tools()agent = create_agent(\\\\\"gpt-4.1\\\\\", tools, context_schema=Context) agent = create_agent(\\\\\"gpt-4.1\\\\\", tools, context_schema =Context) # Invoke with user context # Invoke with user contextresult = await agent.ainvoke(result =  await agent.ainvoke( {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Search my orders\\\\\"}]}, {\\\\\"messages\\\\\": [{\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": \\\\\"Search my orders\\\\\"}]}, context={\\\\\"user_id\\\\\": \\\\\"user_123\\\\\", \\\\\"api_key\\\\\": \\\\\"sk-...\\\\\"}  context ={\\\\\"user_id\\\\\": \\\\\"user_123\\\\\", \\\\\"api_key\\\\\": \\\\\"sk-...\\\\\"})).\",\\n      \"score\": 0.999972,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.66,\\n  \"request_id\": \"c21f6dde-d750-4d93-8491-c691935e4cff\"\\n}', 'id': 'lc_5877ba5b-115c-4870-ac17-e295d9e0371d'}], name='search_web', id='f40c0ffa-a963-4e3f-98a8-214db117e0e9', tool_call_id='call_N9wuwBhgkIOAT2QxXtMjPKqk', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.99999535, 'raw_content': None}, {'url': 'https://composio.dev/blog/langchain-mcp-adapter-a-step-by-step-guide-to-build-mcp-agents', 'title': 'LangChain MCP Adapter: A step-by-step guide to build MCP Agents', 'content': \"The LangChain MCP adapter lets agents connect to any server that follows the Model Context Protocol, including Composio's managed MCP server.\", 'score': 0.9999864, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': 'This module provides functionality to convert MCP prompt messages into LangChain message objects, handling both user and assistant message types. FUNCTION', 'score': 0.9999852, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters - NPM', 'content': 'The library allows you to connect to one or more MCP servers and load tools from them, without needing to manage your own MCP client instances. // Whether to throw on errors if a tool fails to load (optional, default: true). // Whether to prefix tool names with the server name (optional, default: false). // Whether to throw errors if a tool fails to load (optional, default: true). When calling tools from the `camera` MCP server, the following `outputHandling` config will be used:. Similarly, when calling tools on the `microphone` MCP server, the following `outputHandling` config will be used:. You can include a `defaultToolTimeout` field in the server config to set the timeout for all tools for that server, or globally for the entire client by setting it in the top-level config. For secure MCP servers that require OAuth 2.0 authentication, you can use the `authProvider` option instead of manually managing headers. const tools = await client.getTools(); // Only tools from \"working-server\".', 'score': 0.99997294, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/python/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': 'from dataclasses import dataclass from  dataclasses import  dataclassfrom langchain_mcp_adapters.client import MultiServerMCPClient from langchain_mcp_adapters.client import  MultiServerMCPClientfrom langchain_mcp_adapters.interceptors import MCPToolCallRequest from langchain_mcp_adapters.interceptors import  MCPToolCallRequestfrom langchain.agents import create_agent from langchain.agents import  create_agent @dataclass @dataclassclass Context: class  Context: user_id: str user_id: str api_key: str api_key: str async def inject_user_context(async  def  inject_user_context( request: MCPToolCallRequest,  request: MCPToolCallRequest, handler,  handler,):): \"\"\"Inject user credentials into MCP tool calls.\"\"\" \"\"\"Inject user credentials into MCP tool calls.\"\"\" runtime = request.runtime  runtime = request.runtime user_id = runtime.context.user_id  user_id = runtime.context.user_id  api_key = runtime.context.api_key  api_key = runtime.context.api_key   # Add user context to tool arguments  # Add user context to tool arguments modified_request = request.override( modified_request = request.override( args={**request.args, \"user_id\": user_id}  args ={**request.args, \"user_id\": user_id} ) ) return await handler(modified_request)  return  await handler(modified_request) client = MultiServerMCPClient(client = MultiServerMCPClient( {...}, {...}, tool_interceptors=[inject_user_context],  tool_interceptors =[inject_user_context],))tools = await client.get_tools() tools =  await client.get_tools()agent = create_agent(\"gpt-4.1\", tools, context_schema=Context) agent = create_agent(\"gpt-4.1\", tools, context_schema =Context) # Invoke with user context # Invoke with user contextresult = await agent.ainvoke(result =  await agent.ainvoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]}, {\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]}, context={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"}  context ={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"})).', 'score': 0.999972, 'raw_content': None}], 'response_time': 0.66, 'request_id': 'c21f6dde-d750-4d93-8491-c691935e4cff'}}}),\n",
      "              AIMessage(content='Here’s a concise overview of the langchain-mcp-adapters library and how to use it.\\n\\nWhat it is\\n- A set of adapters that let you connect LangChain and LangGraph to MCP (Model Context Protocol) tool servers.\\n- It converts MCP tools into LangChain- and LangGraph-compatible tools, and lets agents pull tools from multiple MCP servers.\\n- It helps you tap into the growing ecosystem of MCP servers without writing custom adapters for each one.\\n\\nWhat you can do with it\\n- Use MCP tool servers across multiple hosts from a single LangChain/LangGraph setup.\\n- Load MCP tools as LangChain tools and feed them into LangChain/LangGraph agents.\\n- Route tool calls across multiple MCP servers and compose tools from different servers in a single agent.\\n- Inject runtime context (like user credentials) into MCP calls via interceptors.\\n\\nLanguages and bindings\\n- Python: langchain_mcp_adapters\\n- JavaScript/TypeScript: @langchain/mcp-adapters (NPM)\\n\\nKey components and concepts\\n- MultiServerMCPClient (Python) / equivalent in JS: Connects to one or more MCP servers and fetches available tools.\\n- MCPToolCallRequest and interceptors: Hooks to modify MCP tool calls (for example, to inject user context or credentials).\\n- Tool loading: get_tools() returns MCP tools converted into LangChain-compatible tool objects.\\n- Context schema: You can define a context model (e.g., user_id, API keys) and pass it to the agent so tool calls have the needed runtime data.\\n\\nTypical usage (high level)\\n- Install:\\n  - Python: pip install langchain-mcp-adapters\\n  - Node: npm i @langchain/mcp-adapters\\n- Create a client that points to your MCP servers, optionally with interceptors to inject context.\\n- Load tools with client.get_tools() and pass them to a LangChain/LangGraph agent.\\n- Run the agent, supplying any runtime context you need (e.g., user_id, api_key).\\n\\nExample (Python, high level)\\n- Define a context schema (e.g., a dataclass with user_id and api_key).\\n- Implement an interceptor to add user context to MCP tool calls.\\n- Build the MultiServerMCPClient with server configs and the interceptor.\\n- tools = await client.get_tools()\\n- agent = create_agent(\"gpt-4.1\", tools, context_schema=Context)\\n- result = await agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"Search my orders\"}]}, context={\"user_id\": \"user_123\", \"api_key\": \"sk-...\"})\\n\\nWhere to read more\\n- MCP Adapters changelog post: “MCP Adapters for LangChain and LangGraph” (overview and motivations)\\n- LangChain Python reference: langchain_mcp_adapters (details on modules like MultiServerMCPClient and interceptors)\\n- LangChain MCP docs: Model Context Protocol (MCP) page (examples and integration patterns)\\n- NPM package: @langchain/mcp-adapters (Node/JS usage and options)\\n- Composio guide: step-by-step MCP adapter usage (hands-on walkthrough)\\n\\nWould you like a concrete minimal example in Python or JavaScript, with a runnable snippet and dummy MCP server config so you can try it end-to-end? If you have a preferred language, I can tailor the example and show exact install commands and code.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2378, 'prompt_tokens': 1675, 'total_tokens': 4053, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1664, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7hcS94iawCQH4oNtXbES9f5J5n0X', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c47a2-2cb3-7c60-8525-f7e1ce9063cc-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1675, 'output_tokens': 2378, 'total_tokens': 4053, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1664}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=Europe/Amsterdam\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='704c56e8-f691-4741-aba8-7c9c6cc5fa24'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 218, 'prompt_tokens': 293, 'total_tokens': 511, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7hguen7Kafuf1QumROCbO4hiNPO7', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c47a6-514d-7401-bd97-95dd4a137868-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'Europe/Amsterdam'}, 'id': 'call_g7J97UUX7hAFgdalrXXgHBcs', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 293, 'output_tokens': 218, 'total_tokens': 511, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Europe/Amsterdam\",\\n  \"datetime\": \"2026-02-10T14:03:36+01:00\",\\n  \"day_of_week\": \"Tuesday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_6c7d5295-4b08-4929-9738-a292f1e90095'}], name='get_current_time', id='d9af31a1-5a65-42e6-9990-a3f71a471f77', tool_call_id='call_g7J97UUX7hAFgdalrXXgHBcs'),\n",
      "              AIMessage(content=\"It's 14:03 (2:03 PM) on Tuesday, February 10, 2026 in Amsterdam (CET, UTC+1).\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 296, 'prompt_tokens': 374, 'total_tokens': 670, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D7hgyAqMtksfaeJfNAoyPzibk5Rkv', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c47a6-73b9-7671-a453-17ebe247743e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 374, 'output_tokens': 296, 'total_tokens': 670, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-foundations (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
